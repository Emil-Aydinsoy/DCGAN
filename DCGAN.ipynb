{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56b49cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn # All nn modules\n",
    "import torch.optim as optim # Optimisation algo like sdg, adam\n",
    "import torchvision.datasets as datasets #has standard datasets that we can call upon\n",
    "import torchvision.transforms as transforms # for data augmantation\n",
    "from torch.utils.data import DataLoader # gives easier data managment and options like minibatches\n",
    "from torch.utils.tensorboard import SummaryWriter #for printing results on tenserboard\n",
    "#from model_units import Discriminator,Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d718ebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,channels_img,features_d):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(channels_img,features_d,kernel_size=4,stride=2,padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(features_d,features_d*2,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(features_d*2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(features_d*2,features_d*4,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(features_d*4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(features_d*4,features_d*8,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(features_d*8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(features_d*8,1,kernel_size=4,stride=2,padding=0),\n",
    "            nn.Sigmoid())\n",
    "    def forward(self,X):\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baf39a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,channels_noise,channels_img,features_g):\n",
    "        super(Generator,self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # N* channels_noise *1 *1\n",
    "            nn.ConvTranspose2d(channels_noise,features_g*16,kernel_size=4,stride=1,padding=0),\n",
    "            nn.BatchNorm2d(features_g*16),\n",
    "            nn.ReLU(),\n",
    "            # N* features_g*16 *4 *4\n",
    "            nn.ConvTranspose2d(features_g*16,features_g*8,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(features_g*8),\n",
    "            nn.ReLU(),    \n",
    "            # N * features*8 * 8 * 8\n",
    "            nn.ConvTranspose2d(features_g*8,features_g*4,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(features_g*4),\n",
    "            nn.ReLU(),\n",
    "            # N * features_g*4 * 16 * 16\n",
    "            nn.ConvTranspose2d(features_g*4,features_g*2,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(features_g*2),\n",
    "            nn.ReLU(),\n",
    "            # N * features_g*2 * 32 * 32\n",
    "            nn.ConvTranspose2d(features_g*2,channels_img,kernel_size=4,stride=2,padding=1),\n",
    "            nn.Tanh()\n",
    "            # N * channels_img * 64 * 64\n",
    "            )\n",
    "    def forward(self,X):\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55b8b68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamtres\n",
    "lr = 0.0002\n",
    "batch_size = 64\n",
    "img_size = 64 # in mnist we have 28*28 so we will need to resize them to 64*64\n",
    "channels_img = 1\n",
    "channels_noise = 256\n",
    "num_epochs = 25\n",
    "features_d = 16\n",
    "features_g = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3ff330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_transforms = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,),(0.5,)),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac494a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.MNIST(root=\"dataset/\",train=True,transform=my_transforms,download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b179135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c1fc610",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a370ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_D = Discriminator(channels_img,features_d).to(device)\n",
    "net_G = Generator(channels_noise,channels_img,features_g).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f403d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seting up optimizers for G and D\n",
    "optimizer_D = optim.Adam(net_D.parameters(),lr=lr, betas=(0.5,0.999))\n",
    "optimizer_G = optim.Adam(net_G.parameters(),lr=lr,betas=(0.5,0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aa4f34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (net): Sequential(\n",
       "    (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU()\n",
       "    (12): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (13): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_D.train()\n",
    "net_G.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "909c2e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting Loss function\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86a51471",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_label = 1\n",
    "fake_label = 0\n",
    "fixed_noise = torch.randn(64,channels_noise,1,1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e998329",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_real = SummaryWriter(f'runs/GAN_MNIST/test_real')\n",
    "writer_fake = SummaryWriter(f'runs/GAN_MNIST/test_fake')    \n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ab82464",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training.....\n",
      "Epoch [0/25] Batch 0/938            Loos D:0.7985,Loss G:1.3535  D(x):0.7519\n",
      "Epoch [0/25] Batch 100/938            Loos D:0.8802,Loss G:0.9905  D(x):0.6344\n",
      "Epoch [0/25] Batch 200/938            Loos D:0.8001,Loss G:1.4022  D(x):0.7193\n",
      "Epoch [0/25] Batch 300/938            Loos D:0.8257,Loss G:1.6954  D(x):0.7679\n",
      "Epoch [0/25] Batch 400/938            Loos D:0.8192,Loss G:1.3863  D(x):0.6820\n",
      "Epoch [0/25] Batch 500/938            Loos D:0.8753,Loss G:2.3062  D(x):0.8991\n",
      "Epoch [0/25] Batch 600/938            Loos D:0.8166,Loss G:1.5446  D(x):0.6847\n",
      "Epoch [0/25] Batch 700/938            Loos D:0.7613,Loss G:1.9751  D(x):0.8402\n",
      "Epoch [0/25] Batch 800/938            Loos D:0.7820,Loss G:1.3144  D(x):0.7226\n",
      "Epoch [0/25] Batch 900/938            Loos D:0.8312,Loss G:1.2998  D(x):0.6769\n",
      "Epoch [1/25] Batch 0/938            Loos D:0.8953,Loss G:1.4509  D(x):0.7318\n",
      "Epoch [1/25] Batch 100/938            Loos D:0.8301,Loss G:1.4681  D(x):0.7917\n",
      "Epoch [1/25] Batch 200/938            Loos D:1.0002,Loss G:0.9191  D(x):0.5408\n",
      "Epoch [1/25] Batch 300/938            Loos D:0.7893,Loss G:1.3779  D(x):0.7221\n",
      "Epoch [1/25] Batch 400/938            Loos D:0.8223,Loss G:2.2897  D(x):0.8834\n",
      "Epoch [1/25] Batch 500/938            Loos D:0.7571,Loss G:1.7240  D(x):0.7936\n",
      "Epoch [1/25] Batch 600/938            Loos D:0.7910,Loss G:1.1647  D(x):0.7237\n",
      "Epoch [1/25] Batch 700/938            Loos D:0.7447,Loss G:1.8811  D(x):0.8163\n",
      "Epoch [1/25] Batch 800/938            Loos D:0.7260,Loss G:1.5361  D(x):0.7825\n",
      "Epoch [1/25] Batch 900/938            Loos D:0.7385,Loss G:1.8889  D(x):0.7925\n",
      "Epoch [2/25] Batch 0/938            Loos D:0.8645,Loss G:1.5357  D(x):0.7266\n",
      "Epoch [2/25] Batch 100/938            Loos D:0.8015,Loss G:1.5894  D(x):0.7335\n",
      "Epoch [2/25] Batch 200/938            Loos D:0.8275,Loss G:1.4235  D(x):0.6879\n",
      "Epoch [2/25] Batch 300/938            Loos D:0.8578,Loss G:2.1585  D(x):0.8126\n",
      "Epoch [2/25] Batch 400/938            Loos D:0.7327,Loss G:1.9077  D(x):0.8102\n",
      "Epoch [2/25] Batch 500/938            Loos D:0.7339,Loss G:1.7938  D(x):0.7821\n",
      "Epoch [2/25] Batch 600/938            Loos D:0.8225,Loss G:2.3568  D(x):0.8516\n",
      "Epoch [2/25] Batch 700/938            Loos D:0.7614,Loss G:2.2791  D(x):0.7841\n",
      "Epoch [2/25] Batch 800/938            Loos D:0.9768,Loss G:1.5057  D(x):0.7425\n",
      "Epoch [2/25] Batch 900/938            Loos D:0.7423,Loss G:1.8051  D(x):0.7873\n",
      "Epoch [3/25] Batch 0/938            Loos D:0.8555,Loss G:3.1382  D(x):0.9243\n",
      "Epoch [3/25] Batch 100/938            Loos D:0.8114,Loss G:2.9434  D(x):0.9115\n",
      "Epoch [3/25] Batch 200/938            Loos D:1.3023,Loss G:3.7534  D(x):0.9537\n",
      "Epoch [3/25] Batch 300/938            Loos D:0.7278,Loss G:1.8928  D(x):0.8098\n",
      "Epoch [3/25] Batch 400/938            Loos D:0.7511,Loss G:1.7426  D(x):0.7801\n",
      "Epoch [3/25] Batch 500/938            Loos D:0.7779,Loss G:1.9551  D(x):0.7322\n",
      "Epoch [3/25] Batch 600/938            Loos D:0.7321,Loss G:2.2168  D(x):0.8199\n",
      "Epoch [3/25] Batch 700/938            Loos D:0.7938,Loss G:1.2042  D(x):0.7156\n",
      "Epoch [3/25] Batch 800/938            Loos D:0.7303,Loss G:1.9322  D(x):0.8092\n",
      "Epoch [3/25] Batch 900/938            Loos D:0.7906,Loss G:1.4854  D(x):0.7512\n",
      "Epoch [4/25] Batch 0/938            Loos D:0.7581,Loss G:2.1447  D(x):0.7892\n",
      "Epoch [4/25] Batch 100/938            Loos D:0.7345,Loss G:1.8899  D(x):0.7899\n",
      "Epoch [4/25] Batch 200/938            Loos D:0.8410,Loss G:1.4248  D(x):0.6599\n",
      "Epoch [4/25] Batch 300/938            Loos D:0.7765,Loss G:2.5422  D(x):0.8819\n",
      "Epoch [4/25] Batch 400/938            Loos D:0.7413,Loss G:2.2801  D(x):0.8697\n",
      "Epoch [4/25] Batch 500/938            Loos D:0.7730,Loss G:2.4231  D(x):0.9064\n",
      "Epoch [4/25] Batch 600/938            Loos D:0.7177,Loss G:1.5676  D(x):0.8198\n",
      "Epoch [4/25] Batch 700/938            Loos D:0.7184,Loss G:1.4880  D(x):0.8044\n",
      "Epoch [4/25] Batch 800/938            Loos D:0.7390,Loss G:1.4872  D(x):0.7857\n",
      "Epoch [4/25] Batch 900/938            Loos D:0.7264,Loss G:2.0297  D(x):0.8351\n",
      "Epoch [5/25] Batch 0/938            Loos D:1.3162,Loss G:3.2777  D(x):0.7840\n",
      "Epoch [5/25] Batch 100/938            Loos D:0.7511,Loss G:2.1647  D(x):0.8572\n",
      "Epoch [5/25] Batch 200/938            Loos D:0.9521,Loss G:2.9698  D(x):0.9299\n",
      "Epoch [5/25] Batch 300/938            Loos D:0.7036,Loss G:2.0229  D(x):0.8641\n",
      "Epoch [5/25] Batch 400/938            Loos D:0.7456,Loss G:1.4294  D(x):0.7615\n",
      "Epoch [5/25] Batch 500/938            Loos D:0.6973,Loss G:2.2775  D(x):0.8364\n",
      "Epoch [5/25] Batch 600/938            Loos D:0.8148,Loss G:1.6259  D(x):0.6905\n",
      "Epoch [5/25] Batch 700/938            Loos D:0.7546,Loss G:1.9314  D(x):0.7619\n",
      "Epoch [5/25] Batch 800/938            Loos D:0.7953,Loss G:1.7543  D(x):0.7113\n",
      "Epoch [5/25] Batch 900/938            Loos D:0.7632,Loss G:1.8274  D(x):0.7940\n",
      "Epoch [6/25] Batch 0/938            Loos D:0.7274,Loss G:2.4210  D(x):0.8971\n",
      "Epoch [6/25] Batch 100/938            Loos D:0.6972,Loss G:2.2247  D(x):0.8708\n",
      "Epoch [6/25] Batch 200/938            Loos D:0.7596,Loss G:2.5581  D(x):0.8877\n",
      "Epoch [6/25] Batch 300/938            Loos D:0.7173,Loss G:2.6636  D(x):0.9124\n",
      "Epoch [6/25] Batch 400/938            Loos D:0.7090,Loss G:2.2538  D(x):0.9268\n",
      "Epoch [6/25] Batch 500/938            Loos D:0.7596,Loss G:1.9399  D(x):0.7699\n",
      "Epoch [6/25] Batch 600/938            Loos D:0.7736,Loss G:1.8633  D(x):0.7289\n",
      "Epoch [6/25] Batch 700/938            Loos D:0.7862,Loss G:1.8229  D(x):0.8113\n",
      "Epoch [6/25] Batch 800/938            Loos D:0.7168,Loss G:1.8128  D(x):0.8430\n",
      "Epoch [6/25] Batch 900/938            Loos D:0.7120,Loss G:1.9109  D(x):0.8211\n",
      "Epoch [7/25] Batch 0/938            Loos D:0.7029,Loss G:1.9209  D(x):0.8329\n",
      "Epoch [7/25] Batch 100/938            Loos D:0.6815,Loss G:2.6048  D(x):0.8824\n",
      "Epoch [7/25] Batch 200/938            Loos D:0.7443,Loss G:2.7735  D(x):0.9381\n",
      "Epoch [7/25] Batch 300/938            Loos D:0.7348,Loss G:2.3650  D(x):0.9216\n",
      "Epoch [7/25] Batch 400/938            Loos D:0.7020,Loss G:2.2304  D(x):0.8143\n",
      "Epoch [7/25] Batch 500/938            Loos D:0.7363,Loss G:1.5877  D(x):0.7708\n",
      "Epoch [7/25] Batch 600/938            Loos D:0.7537,Loss G:1.6009  D(x):0.7470\n",
      "Epoch [7/25] Batch 700/938            Loos D:0.6972,Loss G:1.9359  D(x):0.8325\n",
      "Epoch [7/25] Batch 800/938            Loos D:0.8182,Loss G:1.0286  D(x):0.6883\n",
      "Epoch [7/25] Batch 900/938            Loos D:0.7000,Loss G:2.0884  D(x):0.8304\n",
      "Epoch [8/25] Batch 0/938            Loos D:0.6934,Loss G:2.3749  D(x):0.8489\n",
      "Epoch [8/25] Batch 100/938            Loos D:0.6991,Loss G:1.9426  D(x):0.8325\n",
      "Epoch [8/25] Batch 200/938            Loos D:0.6753,Loss G:2.3662  D(x):0.9105\n",
      "Epoch [8/25] Batch 300/938            Loos D:0.7749,Loss G:1.7715  D(x):0.7431\n",
      "Epoch [8/25] Batch 400/938            Loos D:0.7099,Loss G:2.3833  D(x):0.8607\n",
      "Epoch [8/25] Batch 500/938            Loos D:0.6841,Loss G:2.5112  D(x):0.8705\n",
      "Epoch [8/25] Batch 600/938            Loos D:0.6852,Loss G:2.2476  D(x):0.8739\n",
      "Epoch [8/25] Batch 700/938            Loos D:0.6943,Loss G:2.5925  D(x):0.8787\n",
      "Epoch [8/25] Batch 800/938            Loos D:0.8713,Loss G:1.1499  D(x):0.6992\n",
      "Epoch [8/25] Batch 900/938            Loos D:0.7214,Loss G:1.8971  D(x):0.8115\n",
      "Epoch [9/25] Batch 0/938            Loos D:0.6795,Loss G:2.4859  D(x):0.9316\n",
      "Epoch [9/25] Batch 100/938            Loos D:0.7082,Loss G:2.3720  D(x):0.8863\n",
      "Epoch [9/25] Batch 200/938            Loos D:0.6920,Loss G:2.0530  D(x):0.8649\n",
      "Epoch [9/25] Batch 300/938            Loos D:0.6749,Loss G:1.9833  D(x):0.8572\n",
      "Epoch [9/25] Batch 400/938            Loos D:0.7509,Loss G:2.5969  D(x):0.9263\n",
      "Epoch [9/25] Batch 500/938            Loos D:0.6782,Loss G:2.5257  D(x):0.8851\n",
      "Epoch [9/25] Batch 600/938            Loos D:0.6780,Loss G:2.3743  D(x):0.8905\n",
      "Epoch [9/25] Batch 700/938            Loos D:0.9816,Loss G:1.8145  D(x):0.7935\n",
      "Epoch [9/25] Batch 800/938            Loos D:0.6887,Loss G:1.7984  D(x):0.8447\n",
      "Epoch [9/25] Batch 900/938            Loos D:0.6943,Loss G:2.6414  D(x):0.9189\n",
      "Epoch [10/25] Batch 0/938            Loos D:0.7271,Loss G:1.7152  D(x):0.8148\n",
      "Epoch [10/25] Batch 100/938            Loos D:0.6911,Loss G:1.9119  D(x):0.8477\n",
      "Epoch [10/25] Batch 200/938            Loos D:0.6674,Loss G:2.1846  D(x):0.8911\n",
      "Epoch [10/25] Batch 300/938            Loos D:0.6778,Loss G:2.2415  D(x):0.8722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/25] Batch 400/938            Loos D:0.6798,Loss G:2.3358  D(x):0.8909\n",
      "Epoch [10/25] Batch 500/938            Loos D:0.7419,Loss G:2.5387  D(x):0.9250\n",
      "Epoch [10/25] Batch 600/938            Loos D:0.6994,Loss G:2.7829  D(x):0.9165\n",
      "Epoch [10/25] Batch 700/938            Loos D:0.6775,Loss G:2.6414  D(x):0.8924\n",
      "Epoch [10/25] Batch 800/938            Loos D:0.6805,Loss G:2.5121  D(x):0.9087\n",
      "Epoch [10/25] Batch 900/938            Loos D:0.7127,Loss G:2.3761  D(x):0.8188\n",
      "Epoch [11/25] Batch 0/938            Loos D:0.7169,Loss G:1.4651  D(x):0.7835\n",
      "Epoch [11/25] Batch 100/938            Loos D:0.7467,Loss G:2.0088  D(x):0.8151\n",
      "Epoch [11/25] Batch 200/938            Loos D:0.6959,Loss G:1.9683  D(x):0.8315\n",
      "Epoch [11/25] Batch 300/938            Loos D:0.6775,Loss G:2.1752  D(x):0.8868\n",
      "Epoch [11/25] Batch 400/938            Loos D:0.6992,Loss G:3.0149  D(x):0.9146\n",
      "Epoch [11/25] Batch 500/938            Loos D:0.9625,Loss G:1.1759  D(x):0.5802\n",
      "Epoch [11/25] Batch 600/938            Loos D:0.6841,Loss G:2.0651  D(x):0.8591\n",
      "Epoch [11/25] Batch 700/938            Loos D:0.7658,Loss G:1.4007  D(x):0.7314\n",
      "Epoch [11/25] Batch 800/938            Loos D:0.6668,Loss G:2.4751  D(x):0.9037\n",
      "Epoch [11/25] Batch 900/938            Loos D:0.6887,Loss G:2.1148  D(x):0.8389\n",
      "Epoch [12/25] Batch 0/938            Loos D:0.6855,Loss G:2.5563  D(x):0.9017\n",
      "Epoch [12/25] Batch 100/938            Loos D:0.7238,Loss G:1.6402  D(x):0.7779\n",
      "Epoch [12/25] Batch 200/938            Loos D:0.7088,Loss G:1.7623  D(x):0.7913\n",
      "Epoch [12/25] Batch 300/938            Loos D:0.6961,Loss G:1.7947  D(x):0.8111\n",
      "Epoch [12/25] Batch 400/938            Loos D:0.7260,Loss G:2.2143  D(x):0.7908\n",
      "Epoch [12/25] Batch 500/938            Loos D:0.7371,Loss G:1.7630  D(x):0.7766\n",
      "Epoch [12/25] Batch 600/938            Loos D:0.6886,Loss G:2.0311  D(x):0.8746\n",
      "Epoch [12/25] Batch 700/938            Loos D:0.7857,Loss G:3.1454  D(x):0.9611\n",
      "Epoch [12/25] Batch 800/938            Loos D:0.6812,Loss G:2.0248  D(x):0.8606\n",
      "Epoch [12/25] Batch 900/938            Loos D:0.8992,Loss G:1.1933  D(x):0.6178\n",
      "Epoch [13/25] Batch 0/938            Loos D:0.7996,Loss G:1.4348  D(x):0.7362\n",
      "Epoch [13/25] Batch 100/938            Loos D:0.6834,Loss G:2.0269  D(x):0.8893\n",
      "Epoch [13/25] Batch 200/938            Loos D:0.7389,Loss G:2.6305  D(x):0.9354\n",
      "Epoch [13/25] Batch 300/938            Loos D:0.6939,Loss G:2.1913  D(x):0.8372\n",
      "Epoch [13/25] Batch 400/938            Loos D:0.6657,Loss G:2.1810  D(x):0.8982\n",
      "Epoch [13/25] Batch 500/938            Loos D:0.6681,Loss G:2.3464  D(x):0.9071\n",
      "Epoch [13/25] Batch 600/938            Loos D:0.6849,Loss G:2.4195  D(x):0.8916\n",
      "Epoch [13/25] Batch 700/938            Loos D:0.6684,Loss G:2.2687  D(x):0.8768\n",
      "Epoch [13/25] Batch 800/938            Loos D:0.6736,Loss G:2.3814  D(x):0.8898\n",
      "Epoch [13/25] Batch 900/938            Loos D:0.7625,Loss G:1.6725  D(x):0.7611\n",
      "Epoch [14/25] Batch 0/938            Loos D:1.2362,Loss G:2.2191  D(x):0.9303\n",
      "Epoch [14/25] Batch 100/938            Loos D:0.6844,Loss G:2.3930  D(x):0.9051\n",
      "Epoch [14/25] Batch 200/938            Loos D:0.6830,Loss G:2.0813  D(x):0.8495\n",
      "Epoch [14/25] Batch 300/938            Loos D:0.7056,Loss G:2.0564  D(x):0.8145\n",
      "Epoch [14/25] Batch 400/938            Loos D:0.6889,Loss G:2.7345  D(x):0.9036\n",
      "Epoch [14/25] Batch 500/938            Loos D:0.6678,Loss G:2.5885  D(x):0.9090\n",
      "Epoch [14/25] Batch 600/938            Loos D:0.7281,Loss G:1.5873  D(x):0.7847\n",
      "Epoch [14/25] Batch 700/938            Loos D:0.6743,Loss G:2.2780  D(x):0.8669\n",
      "Epoch [14/25] Batch 800/938            Loos D:0.6822,Loss G:2.6444  D(x):0.9121\n",
      "Epoch [14/25] Batch 900/938            Loos D:0.7549,Loss G:3.0061  D(x):0.9350\n",
      "Epoch [15/25] Batch 0/938            Loos D:0.6727,Loss G:2.7151  D(x):0.9215\n",
      "Epoch [15/25] Batch 100/938            Loos D:0.6802,Loss G:2.0189  D(x):0.8472\n",
      "Epoch [15/25] Batch 200/938            Loos D:0.6669,Loss G:2.3703  D(x):0.8838\n",
      "Epoch [15/25] Batch 300/938            Loos D:0.6914,Loss G:1.8315  D(x):0.8299\n",
      "Epoch [15/25] Batch 400/938            Loos D:0.7085,Loss G:2.4314  D(x):0.8095\n",
      "Epoch [15/25] Batch 500/938            Loos D:0.6733,Loss G:2.2530  D(x):0.8630\n",
      "Epoch [15/25] Batch 600/938            Loos D:0.7348,Loss G:2.8257  D(x):0.9193\n",
      "Epoch [15/25] Batch 700/938            Loos D:0.6679,Loss G:2.1804  D(x):0.8684\n",
      "Epoch [15/25] Batch 800/938            Loos D:0.6907,Loss G:2.0426  D(x):0.8366\n",
      "Epoch [15/25] Batch 900/938            Loos D:0.7129,Loss G:2.4142  D(x):0.9512\n",
      "Epoch [16/25] Batch 0/938            Loos D:0.6637,Loss G:2.4637  D(x):0.9078\n",
      "Epoch [16/25] Batch 100/938            Loos D:0.6810,Loss G:2.2443  D(x):0.8697\n",
      "Epoch [16/25] Batch 200/938            Loos D:0.6929,Loss G:2.0417  D(x):0.8254\n",
      "Epoch [16/25] Batch 300/938            Loos D:0.7194,Loss G:2.1282  D(x):0.7959\n",
      "Epoch [16/25] Batch 400/938            Loos D:0.8421,Loss G:3.0363  D(x):0.9264\n",
      "Epoch [16/25] Batch 500/938            Loos D:0.6877,Loss G:2.4287  D(x):0.8986\n",
      "Epoch [16/25] Batch 600/938            Loos D:0.6839,Loss G:1.9353  D(x):0.8404\n",
      "Epoch [16/25] Batch 700/938            Loos D:0.6630,Loss G:2.2746  D(x):0.9053\n",
      "Epoch [16/25] Batch 800/938            Loos D:0.6714,Loss G:2.3747  D(x):0.8715\n",
      "Epoch [16/25] Batch 900/938            Loos D:0.6684,Loss G:2.5161  D(x):0.9059\n",
      "Epoch [17/25] Batch 0/938            Loos D:1.0061,Loss G:0.8966  D(x):0.5511\n",
      "Epoch [17/25] Batch 100/938            Loos D:0.6826,Loss G:1.7630  D(x):0.8348\n",
      "Epoch [17/25] Batch 200/938            Loos D:0.6774,Loss G:2.1227  D(x):0.8499\n",
      "Epoch [17/25] Batch 300/938            Loos D:0.6784,Loss G:1.8632  D(x):0.8594\n",
      "Epoch [17/25] Batch 400/938            Loos D:0.6630,Loss G:2.7359  D(x):0.9097\n",
      "Epoch [17/25] Batch 500/938            Loos D:0.6658,Loss G:2.2287  D(x):0.8754\n",
      "Epoch [17/25] Batch 600/938            Loos D:0.6635,Loss G:2.3309  D(x):0.8874\n",
      "Epoch [17/25] Batch 700/938            Loos D:0.6702,Loss G:1.8878  D(x):0.8699\n",
      "Epoch [17/25] Batch 800/938            Loos D:0.7278,Loss G:1.9253  D(x):0.7691\n",
      "Epoch [17/25] Batch 900/938            Loos D:0.6712,Loss G:2.3656  D(x):0.9175\n",
      "Epoch [18/25] Batch 0/938            Loos D:0.8159,Loss G:4.0696  D(x):0.9637\n",
      "Epoch [18/25] Batch 100/938            Loos D:0.6707,Loss G:2.3922  D(x):0.8714\n",
      "Epoch [18/25] Batch 200/938            Loos D:0.6771,Loss G:2.4326  D(x):0.8976\n",
      "Epoch [18/25] Batch 300/938            Loos D:0.6651,Loss G:2.2574  D(x):0.8868\n",
      "Epoch [18/25] Batch 400/938            Loos D:0.6670,Loss G:2.3013  D(x):0.8805\n",
      "Epoch [18/25] Batch 500/938            Loos D:0.6710,Loss G:2.2119  D(x):0.8776\n",
      "Epoch [18/25] Batch 600/938            Loos D:0.6816,Loss G:2.6505  D(x):0.8929\n",
      "Epoch [18/25] Batch 700/938            Loos D:0.7106,Loss G:2.9837  D(x):0.9288\n",
      "Epoch [18/25] Batch 800/938            Loos D:0.7427,Loss G:2.5654  D(x):0.9050\n",
      "Epoch [18/25] Batch 900/938            Loos D:0.6640,Loss G:2.2416  D(x):0.9048\n",
      "Epoch [19/25] Batch 0/938            Loos D:0.6655,Loss G:2.4356  D(x):0.8841\n",
      "Epoch [19/25] Batch 100/938            Loos D:0.7138,Loss G:2.8926  D(x):0.9523\n",
      "Epoch [19/25] Batch 200/938            Loos D:0.6748,Loss G:2.2571  D(x):0.8800\n",
      "Epoch [19/25] Batch 300/938            Loos D:0.6668,Loss G:2.6786  D(x):0.9224\n",
      "Epoch [19/25] Batch 400/938            Loos D:0.6686,Loss G:2.2287  D(x):0.8631\n",
      "Epoch [19/25] Batch 500/938            Loos D:0.6867,Loss G:2.5916  D(x):0.9367\n",
      "Epoch [19/25] Batch 600/938            Loos D:0.6899,Loss G:2.1069  D(x):0.8336\n",
      "Epoch [19/25] Batch 700/938            Loos D:0.7810,Loss G:2.6050  D(x):0.8717\n",
      "Epoch [19/25] Batch 800/938            Loos D:0.6632,Loss G:2.3075  D(x):0.9006\n",
      "Epoch [19/25] Batch 900/938            Loos D:0.6639,Loss G:2.3103  D(x):0.8824\n",
      "Epoch [20/25] Batch 0/938            Loos D:0.6702,Loss G:2.0563  D(x):0.8805\n",
      "Epoch [20/25] Batch 100/938            Loos D:0.6750,Loss G:2.2299  D(x):0.8655\n",
      "Epoch [20/25] Batch 200/938            Loos D:0.6656,Loss G:2.3523  D(x):0.8792\n",
      "Epoch [20/25] Batch 300/938            Loos D:0.6648,Loss G:2.2395  D(x):0.9227\n",
      "Epoch [20/25] Batch 400/938            Loos D:0.9154,Loss G:1.1079  D(x):0.6299\n",
      "Epoch [20/25] Batch 500/938            Loos D:0.6759,Loss G:2.4410  D(x):0.9033\n",
      "Epoch [20/25] Batch 600/938            Loos D:0.7236,Loss G:2.0200  D(x):0.8019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/25] Batch 700/938            Loos D:0.6791,Loss G:1.9027  D(x):0.8408\n",
      "Epoch [20/25] Batch 800/938            Loos D:0.6946,Loss G:1.9991  D(x):0.8161\n",
      "Epoch [20/25] Batch 900/938            Loos D:0.6677,Loss G:2.5583  D(x):0.8616\n",
      "Epoch [21/25] Batch 0/938            Loos D:0.6641,Loss G:2.6229  D(x):0.9124\n",
      "Epoch [21/25] Batch 100/938            Loos D:0.6644,Loss G:2.3034  D(x):0.8706\n",
      "Epoch [21/25] Batch 200/938            Loos D:0.6656,Loss G:2.2393  D(x):0.8665\n",
      "Epoch [21/25] Batch 300/938            Loos D:0.6652,Loss G:2.5590  D(x):0.9025\n",
      "Epoch [21/25] Batch 400/938            Loos D:0.6900,Loss G:2.9620  D(x):0.9403\n",
      "Epoch [21/25] Batch 500/938            Loos D:0.6798,Loss G:2.7405  D(x):0.9001\n",
      "Epoch [21/25] Batch 600/938            Loos D:0.6819,Loss G:2.5057  D(x):0.9212\n",
      "Epoch [21/25] Batch 700/938            Loos D:0.7143,Loss G:1.6027  D(x):0.7753\n",
      "Epoch [21/25] Batch 800/938            Loos D:0.8241,Loss G:1.7479  D(x):0.7083\n",
      "Epoch [21/25] Batch 900/938            Loos D:0.6824,Loss G:2.1124  D(x):0.8811\n",
      "Epoch [22/25] Batch 0/938            Loos D:0.7311,Loss G:2.6424  D(x):0.9376\n",
      "Epoch [22/25] Batch 100/938            Loos D:0.6926,Loss G:2.2376  D(x):0.8750\n",
      "Epoch [22/25] Batch 200/938            Loos D:0.6641,Loss G:2.3466  D(x):0.8752\n",
      "Epoch [22/25] Batch 300/938            Loos D:0.6657,Loss G:2.0901  D(x):0.8905\n",
      "Epoch [22/25] Batch 400/938            Loos D:0.6620,Loss G:2.4715  D(x):0.8989\n",
      "Epoch [22/25] Batch 500/938            Loos D:0.6647,Loss G:2.3917  D(x):0.8906\n",
      "Epoch [22/25] Batch 600/938            Loos D:0.6715,Loss G:1.9951  D(x):0.8605\n",
      "Epoch [22/25] Batch 700/938            Loos D:0.6651,Loss G:2.2310  D(x):0.8902\n",
      "Epoch [22/25] Batch 800/938            Loos D:0.7793,Loss G:3.3760  D(x):0.9511\n",
      "Epoch [22/25] Batch 900/938            Loos D:0.6708,Loss G:2.2169  D(x):0.8581\n",
      "Epoch [23/25] Batch 0/938            Loos D:0.6655,Loss G:2.3579  D(x):0.8736\n",
      "Epoch [23/25] Batch 100/938            Loos D:0.6938,Loss G:2.0204  D(x):0.8151\n",
      "Epoch [23/25] Batch 200/938            Loos D:0.6600,Loss G:2.4715  D(x):0.9068\n",
      "Epoch [23/25] Batch 300/938            Loos D:0.6705,Loss G:2.4357  D(x):0.9280\n",
      "Epoch [23/25] Batch 400/938            Loos D:1.1115,Loss G:2.5826  D(x):0.6789\n",
      "Epoch [23/25] Batch 500/938            Loos D:0.6923,Loss G:2.4723  D(x):0.9036\n",
      "Epoch [23/25] Batch 600/938            Loos D:0.6802,Loss G:1.9119  D(x):0.8401\n",
      "Epoch [23/25] Batch 700/938            Loos D:0.6746,Loss G:1.8955  D(x):0.8523\n",
      "Epoch [23/25] Batch 800/938            Loos D:0.6892,Loss G:2.2281  D(x):0.8309\n",
      "Epoch [23/25] Batch 900/938            Loos D:0.6614,Loss G:2.2962  D(x):0.8898\n",
      "Epoch [24/25] Batch 0/938            Loos D:0.7269,Loss G:2.7703  D(x):0.9565\n",
      "Epoch [24/25] Batch 100/938            Loos D:0.6734,Loss G:2.3116  D(x):0.8555\n",
      "Epoch [24/25] Batch 200/938            Loos D:0.6805,Loss G:2.6042  D(x):0.9048\n",
      "Epoch [24/25] Batch 300/938            Loos D:0.6713,Loss G:2.3950  D(x):0.9323\n",
      "Epoch [24/25] Batch 400/938            Loos D:0.6655,Loss G:2.1874  D(x):0.8679\n",
      "Epoch [24/25] Batch 500/938            Loos D:1.5245,Loss G:2.9381  D(x):0.8229\n",
      "Epoch [24/25] Batch 600/938            Loos D:0.6645,Loss G:2.5745  D(x):0.9209\n",
      "Epoch [24/25] Batch 700/938            Loos D:0.6762,Loss G:2.0787  D(x):0.8563\n",
      "Epoch [24/25] Batch 800/938            Loos D:0.6623,Loss G:2.3545  D(x):0.9002\n",
      "Epoch [24/25] Batch 900/938            Loos D:0.6672,Loss G:2.4959  D(x):0.9003\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training.....\")\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx,(data,targets) in enumerate(dataloader):\n",
    "        data = data.to(device)\n",
    "        batch_size = data.shape[0]\n",
    "        #Train Discriminator: max log(D(x)) + log(1-D(G(z)))\n",
    "        net_D.zero_grad()\n",
    "        label = (torch.ones(batch_size)*0.9).to(device)\n",
    "        output = net_D(data).reshape(-1)\n",
    "        loss_D_real = criterion(output,label)\n",
    "        D_x = output.mean().item()\n",
    "        \n",
    "        noise = torch.randn(batch_size,channels_noise,1,1).to(device)\n",
    "        fake = net_G(noise)\n",
    "        label = (torch.ones(batch_size)*0.1).to(device)\n",
    "        output = net_D(fake.detach()).reshape(-1)\n",
    "        loss_D_fake = criterion(output,label)\n",
    "        \n",
    "        loss_D = loss_D_real+loss_D_fake\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "        # Train Generator: max log(D(G(z)))\n",
    "        net_G.zero_grad()\n",
    "        label = torch.ones(batch_size).to(device)\n",
    "        output = net_D(fake).reshape(-1)\n",
    "        loss_G = criterion(output,label)\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        if batch_idx % 100==0:\n",
    "            print(f'Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(dataloader)}\\\n",
    "            Loos D:{loss_D:.4f},Loss G:{loss_G:.4f}  D(x):{D_x:.4f}')            \n",
    "            with torch.no_grad():\n",
    "                fake = net_G(fixed_noise)\n",
    "                img_grid_real = torchvision.utils.make_grid(data[:32],normalize=True)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake[:32],normalize=True)\n",
    "                writer_real.add_image(\"MNIST Real Images\",img_grid_real,global_step=step)\n",
    "                writer_fake.add_image(\"MNIST Fake Images\",img_grid_fake,global_step=step)\n",
    "                step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7125959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
